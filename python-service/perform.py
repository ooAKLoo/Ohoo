# server.py - Paraformerç‰ˆæœ¬
import os
import gc
import sys
import time
import asyncio
import tempfile
import logging
from pathlib import Path
from typing import Optional
from contextlib import asynccontextmanager

# å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
sys.stdout.reconfigure(line_buffering=True)

import torch
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from funasr import AutoModel

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

app = FastAPI(title="Paraformer Speech Recognition API")

# æ·»åŠ CORSæ”¯æŒ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ¨¡å‹å˜é‡å’Œä¿¡å·é‡
model = None
# é™åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
SEMAPHORE = asyncio.Semaphore(3)


def init_model():
    """åˆå§‹åŒ–Paraformeræ¨¡å‹"""
    global model
    try:
        print("=" * 70, flush=True)
        print("ğŸš€ å¼€å§‹åˆå§‹åŒ– Paraformer è¯­éŸ³è¯†åˆ«æ¨¡å‹", flush=True)
        print("ğŸ“¢ Paraformerç‰¹ç‚¹ï¼šä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡é«˜ï¼Œæ”¯æŒçƒ­è¯åŠŸèƒ½", flush=True)
        print("âš ï¸  é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¯·è€å¿ƒç­‰å¾…...", flush=True)
        print("=" * 70, flush=True)
        
        # æ£€æµ‹è®¾å¤‡
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}", flush=True)

        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_dir = Path.home() / ".cache" / "modelscope"
        print(f"ğŸ“‚ æ¨¡å‹ç¼“å­˜ç›®å½•: {cache_dir}", flush=True)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
        model_path = cache_dir / "hub" / "damo" / "speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
        if model_path.exists():
            print("âœ… æ£€æµ‹åˆ°å·²ä¸‹è½½çš„Paraformeræ¨¡å‹æ–‡ä»¶", flush=True)
        else:
            print("â¬‡ï¸  å¼€å§‹ä¸‹è½½Paraformeræ¨¡å‹æ–‡ä»¶ï¼Œè¯·ä¿æŒç½‘ç»œè¿æ¥...", flush=True)

        # è®¾ç½®PyTorchæ€§èƒ½ä¼˜åŒ–
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.cuda.empty_cache()

        # åˆå§‹åŒ–Paraformeræ¨¡å‹
        print("â³ æ­£åœ¨åŠ è½½Paraformeræ¨¡å‹åˆ°å†…å­˜ï¼Œè¯·ç¨å€™...", flush=True)
        start_time = time.time()
        
        # Paraformeræ¨¡å‹é…ç½®
        model = AutoModel(
            model="paraformer-zh",  # ä½¿ç”¨ä¸­æ–‡Paraformeræ¨¡å‹
            vad_model="fsmn-vad",   # VADæ¨¡å‹ç”¨äºé•¿éŸ³é¢‘åˆ‡å‰²
            punc_model="ct-punc",    # æ ‡ç‚¹æ¢å¤æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            # spk_model="cam++",     # è¯´è¯äººåˆ†ç¦»æ¨¡å‹ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦å¼€å¯ï¼‰
            vad_kwargs={
                "max_single_segment_time": 30000,  # VADæœ€å¤§åˆ‡å‰²æ—¶é•¿30ç§’
                "speech_noise_threshold": 0.8,     # è¯­éŸ³å™ªå£°é˜ˆå€¼
            },
            device=device,
            disable_update=True,
            ncpu=4,  # CPUçº¿ç¨‹æ•°
        )
        
        load_time = time.time() - start_time
        print(f"â±ï¸  Paraformeræ¨¡å‹åŠ è½½å®Œæˆï¼è€—æ—¶: {load_time:.2f} ç§’", flush=True)

        # æ¨¡å‹æ¨ç†ä¼˜åŒ–
        if hasattr(model, 'model') and hasattr(model.model, 'eval'):
            model.model.eval()
        
        print("=" * 70, flush=True)
        print("âœ… Paraformeræ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼æœåŠ¡å‡†å¤‡å°±ç»ª", flush=True)
        print("ğŸŒ APIåœ°å€: http://localhost:8001", flush=True)
        print("ğŸ’¡ æç¤ºï¼šParaformerå¯¹ä¸­æ–‡è¯†åˆ«æ•ˆæœæœ€ä½³", flush=True)
        print("=" * 70, flush=True)
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}", flush=True)
        logger.error(f"Failed to initialize model: {e}")
        return False


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨¡å‹"""
    if not init_model():
        logger.error("Failed to initialize Paraformer model")


@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model": "Paraformer-zh",
        "model_loaded": model is not None,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "features": ["çƒ­è¯æ”¯æŒ", "ä¸­æ–‡ä¼˜åŒ–", "æ ‡ç‚¹æ¢å¤", "VADåˆ‡å‰²"]
    }


async def cleanup_temp_file(file_path: str):
    """å¼‚æ­¥æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    await asyncio.sleep(1)
    try:
        os.unlink(file_path)
    except:
        pass


# å¸¸ç”¨çƒ­è¯é…ç½®
DEFAULT_HOTWORDS = {
    "tech": "å¾®ä¿¡ æ”¯ä»˜å® æ·˜å® ChatGPT OpenAI API Python JavaScript",
    "general": "çš„ äº† å— å•Š å‘¢ å§",  # å¸¸ç”¨è¯­æ°”è¯
    "business": "ä¼šè®® æŠ¥å‘Š é¡¹ç›® å®¢æˆ· äº§å“ å¸‚åœº",
}


def post_process_text(text):
    """åå¤„ç†ï¼šä¿®æ­£å¸¸è§é”™è¯¯"""
    # å®šä¹‰å¸¸è§é”™è¯¯æ˜ å°„
    corrections = {
        # å“ç‰Œ/äº§å“å
        'we chat': 'WeChat',
        'å¾® ä¿¡': 'å¾®ä¿¡',
        'chat gpt': 'ChatGPT',
        'chat GPT': 'ChatGPT',
        
        # å¸¸è§ä¸­è‹±æ··åˆ
        'APP': 'App',
        'iphone': 'iPhone',
        'ipad': 'iPad',
        
        # è¯­éŸ³è¯†åˆ«å¸¸è§é”™è¯¯
        'çš„è¯': 'çš„è¯',  # é˜²æ­¢è¢«åˆ†å¼€
        'é‚£ä¹ˆ': 'é‚£ä¹ˆ',
    }
    
    for wrong, correct in corrections.items():
        text = text.replace(wrong, correct)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())
    
    return text


@app.post("/transcribe/normal")
async def transcribe_normal(
        background_tasks: BackgroundTasks,
        file: UploadFile = File(...),
        language: Optional[str] = Form("auto"),
        use_itn: Optional[bool] = Form(True),
        scenario: Optional[str] = Form("mixed"),  # åœºæ™¯å‚æ•°
        hotwords: Optional[str] = Form("")  # çƒ­è¯å‚æ•°
):
    """
    è½¬å½•æ¥å£ - Paraformerç‰ˆæœ¬
    æ”¯æŒçƒ­è¯åŠŸèƒ½ï¼Œä¸­æ–‡è¯†åˆ«æ•ˆæœæ›´å¥½
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    # æ–‡ä»¶å¤§å°é™åˆ¶
    file_size = 0
    contents = b""
    chunk_size = 8192
    max_size = 100 * 1024 * 1024  # 100MB
    
    while True:
        chunk = await file.read(chunk_size)
        if not chunk:
            break
        file_size += len(chunk)
        if file_size > max_size:
            raise HTTPException(status_code=413, detail="File too large")
        contents += chunk

    # ä¿¡å·é‡é™åˆ¶å¹¶å‘
    async with SEMAPHORE:
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp:
            tmp.write(contents)
            tmp_path = tmp.name

        # æ·»åŠ åå°ä»»åŠ¡æ¸…ç†æ–‡ä»¶
        background_tasks.add_task(cleanup_temp_file, tmp_path)

        try:
            # å‡†å¤‡çƒ­è¯
            final_hotwords = hotwords
            if not final_hotwords and scenario in ["mixed", "chinese"]:
                # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰çƒ­è¯ï¼Œä½¿ç”¨é»˜è®¤çƒ­è¯
                final_hotwords = DEFAULT_HOTWORDS.get("tech", "")
            
            print(f"ğŸ”¥ ä½¿ç”¨çƒ­è¯: {final_hotwords[:50]}..." if final_hotwords else "æœªä½¿ç”¨çƒ­è¯", flush=True)
            
            # ä½¿ç”¨torch.no_grad()å‡å°‘å†…å­˜å ç”¨
            with torch.no_grad():
                res = model.generate(
                    input=tmp_path,
                    batch_size_s=300,  # Paraformerå¯ä»¥å¤„ç†æ›´é•¿çš„æ‰¹æ¬¡
                    batch_size_threshold_s=60,  # è¶…è¿‡60ç§’çš„éŸ³é¢‘å•ç‹¬å¤„ç†
                    hotword=final_hotwords,  # åº”ç”¨çƒ­è¯
                    # Paraformerç‰¹æœ‰çš„å‚æ•°
                    decoding_ctc_weight=0.0,  # CTCæƒé‡ï¼Œ0è¡¨ç¤ºåªç”¨attention
                    beam_size=10,  # beam searchå¤§å°
                )

            if res and len(res) > 0:
                # æå–æ–‡æœ¬ç»“æœ
                text = res[0].get('text', '') if isinstance(res[0], dict) else str(res[0])
                
                # åå¤„ç†ä¼˜åŒ–
                text = post_process_text(text)
                
                # ä¸»åŠ¨æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # æ—¥å¿—è®°å½•è¯†åˆ«ç»“æœé•¿åº¦
                logger.info(f"è¯†åˆ«å®Œæˆï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                    
                return {
                    "text": text,
                    "filename": file.filename,
                    "model": "Paraformer",
                    "hotwords_used": bool(final_hotwords),
                    "language": "zh"  # Paraformerä¸»è¦ç”¨äºä¸­æ–‡
                }
            else:
                raise HTTPException(status_code=500, detail="No transcription result")

        except Exception as e:
            logger.error(f"Transcription error: {e}")
            raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")


# æ–°å¢ï¼šæµå¼è¯†åˆ«æ¥å£ï¼ˆå¯é€‰ï¼‰
@app.post("/transcribe/streaming")
async def transcribe_streaming(
        file: UploadFile = File(...),
        chunk_size: str = Form("0,10,5"),  # æµå¼é…ç½®
        hotwords: Optional[str] = Form("")
):
    """
    æµå¼è¯†åˆ«æ¥å£ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰
    ä½¿ç”¨paraformer-zh-streamingæ¨¡å‹
    """
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµå¼è¯†åˆ«çš„å®ç°
    # éœ€è¦ä½¿ç”¨ paraformer-zh-streaming æ¨¡å‹
    return {"message": "Streaming API is experimental", "status": "not_implemented"}


if __name__ == "__main__":
    import uvicorn
    print("\n" + "ğŸ¤" * 35 + "\n", flush=True)
    print("ğŸš€ å¯åŠ¨ Ohoo Paraformer è¯­éŸ³è¯†åˆ«æœåŠ¡...", flush=True)
    print("ğŸ“ Paraformerç‰ˆæœ¬ï¼šä¸“æ³¨ä¸­æ–‡è¯†åˆ«ï¼Œæ”¯æŒçƒ­è¯åŠŸèƒ½", flush=True)
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    print("\n" + "ğŸ¤" * 35 + "\n", flush=True)
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8001,
        log_level="info",
        access_log=True
    )