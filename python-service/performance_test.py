#!/usr/bin/env python3
# coding: utf-8
"""
SenseVoice æ€§èƒ½å¯¹æ¯”æµ‹è¯•å·¥å…·
å¯¹æ¯” PyTorch (.pt) æ¨¡å‹ vs ONNX æ¨¡å‹çš„æ€§èƒ½å·®å¼‚

æµ‹è¯•æŒ‡æ ‡ï¼š
1. æœåŠ¡å¯åŠ¨æ—¶é—´
2. æ¨¡å‹åŠ è½½æ—¶é—´  
3. é¦–æ¬¡æ¨ç†æ—¶é—´ï¼ˆå†·å¯åŠ¨ï¼‰
4. å¹³å‡æ¨ç†æ—¶é—´ï¼ˆçƒ­å¯åŠ¨ï¼‰
5. å†…å­˜ä½¿ç”¨æƒ…å†µ
6. CPUä½¿ç”¨æƒ…å†µ
"""

import os
import sys
import time
import json
import requests
import subprocess
import threading
import psutil
from pathlib import Path
from typing import Dict, List
import matplotlib.pyplot as plt
import pandas as pd

class PerformanceMonitor:
    def __init__(self):
        self.cpu_usage = []
        self.memory_usage = []
        self.timestamps = []
        self.monitoring = False
        self.process = None
    
    def start_monitoring(self, pid):
        """å¼€å§‹ç›‘æ§è¿›ç¨‹æ€§èƒ½"""
        self.process = psutil.Process(pid)
        self.monitoring = True
        self.cpu_usage = []
        self.memory_usage = []
        self.timestamps = []
        
        def monitor():
            start_time = time.time()
            while self.monitoring:
                try:
                    cpu = self.process.cpu_percent()
                    memory = self.process.memory_info().rss / 1024 / 1024  # MB
                    timestamp = time.time() - start_time
                    
                    self.cpu_usage.append(cpu)
                    self.memory_usage.append(memory)
                    self.timestamps.append(timestamp)
                    
                    time.sleep(0.5)  # 0.5ç§’é‡‡æ ·ä¸€æ¬¡
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    break
        
        self.monitor_thread = threading.Thread(target=monitor)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        return {
            'cpu_usage': self.cpu_usage,
            'memory_usage': self.memory_usage,
            'timestamps': self.timestamps,
            'avg_cpu': sum(self.cpu_usage) / len(self.cpu_usage) if self.cpu_usage else 0,
            'max_memory': max(self.memory_usage) if self.memory_usage else 0,
            'avg_memory': sum(self.memory_usage) / len(self.memory_usage) if self.memory_usage else 0
        }

class ServerTester:
    def __init__(self, script_path: str, server_name: str, port: int = 8001):
        self.script_path = script_path
        self.server_name = server_name
        self.port = port
        self.base_url = f"http://localhost:{port}"
        self.process = None
        self.monitor = PerformanceMonitor()
        
    def start_server(self) -> Dict:
        """å¯åŠ¨æœåŠ¡å™¨å¹¶æµ‹é‡å¯åŠ¨æ—¶é—´"""
        print(f"\nğŸš€ å¯åŠ¨ {self.server_name} æœåŠ¡å™¨...")
        start_time = time.time()
        
        # å¯åŠ¨æœåŠ¡å™¨è¿›ç¨‹
        env = os.environ.copy()
        env['PYTHONPATH'] = str(Path(self.script_path).parent)
        
        self.process = subprocess.Popen(
            [sys.executable, self.script_path],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            env=env,
            cwd=Path(self.script_path).parent
        )
        
        # å¼€å§‹æ€§èƒ½ç›‘æ§
        self.monitor.start_monitoring(self.process.pid)
        
        # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        model_load_time = None
        startup_complete = False
        
        while True:
            line = self.process.stdout.readline()
            if not line:
                break
                
            print(f"[{self.server_name}] {line.strip()}")
            
            # æ£€æµ‹æ¨¡å‹åŠ è½½å®Œæˆ
            if "æ¨¡å‹åŠ è½½å®Œæˆ" in line and "è€—æ—¶" in line:
                # æå–åŠ è½½æ—¶é—´
                import re
                match = re.search(r'è€—æ—¶: ([\d.]+) ç§’', line)
                if match:
                    model_load_time = float(match.group(1))
            
            # æ£€æµ‹æœåŠ¡å™¨å¯åŠ¨å®Œæˆ
            if "Uvicorn running on" in line:
                startup_time = time.time() - start_time
                startup_complete = True
                break
                
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å¼‚å¸¸é€€å‡º
            if self.process.poll() is not None:
                print(f"âŒ {self.server_name} æœåŠ¡å™¨å¯åŠ¨å¤±è´¥!")
                return None
        
        if not startup_complete:
            print(f"âŒ {self.server_name} æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶!")
            return None
        
        # ç­‰å¾…æœåŠ¡å™¨å®Œå…¨å°±ç»ª
        self._wait_for_health_check()
        
        total_startup_time = time.time() - start_time
        
        return {
            'startup_time': total_startup_time,
            'model_load_time': model_load_time,
            'server_ready': True
        }
    
    def _wait_for_health_check(self, timeout: int = 30):
        """ç­‰å¾…æœåŠ¡å™¨å¥åº·æ£€æŸ¥é€šè¿‡"""
        print(f"â³ ç­‰å¾… {self.server_name} æœåŠ¡å™¨å°±ç»ª...")
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"{self.base_url}/", timeout=5)
                if response.status_code == 200:
                    print(f"âœ… {self.server_name} æœåŠ¡å™¨å°±ç»ª!")
                    return True
            except requests.exceptions.RequestException:
                pass
            time.sleep(1)
        
        raise Exception(f"{self.server_name} æœåŠ¡å™¨å¥åº·æ£€æŸ¥å¤±è´¥!")
    
    def test_inference(self, audio_file: str, num_tests: int = 5) -> Dict:
        """æµ‹è¯•æ¨ç†æ€§èƒ½"""
        print(f"\nğŸ§ª æµ‹è¯• {self.server_name} æ¨ç†æ€§èƒ½ (å…±{num_tests}æ¬¡)...")
        
        if not Path(audio_file).exists():
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        
        results = []
        
        for i in range(num_tests):
            print(f"  æµ‹è¯• {i+1}/{num_tests}...")
            start_time = time.time()
            
            try:
                with open(audio_file, 'rb') as f:
                    files = {'file': f}
                    data = {'language': 'auto', 'use_itn': True}
                    
                    response = requests.post(
                        f"{self.base_url}/transcribe/normal",
                        files=files,
                        data=data,
                        timeout=60
                    )
                
                inference_time = time.time() - start_time
                
                if response.status_code == 200:
                    result = response.json()
                    results.append({
                        'inference_time': inference_time,
                        'text': result.get('text', ''),
                        'success': True
                    })
                    print(f"    âœ… è€—æ—¶: {inference_time:.2f}s")
                    print(f"    ğŸ“ ç»“æœ: {result.get('text', '')[:50]}...")
                else:
                    results.append({
                        'inference_time': inference_time,
                        'text': '',
                        'success': False,
                        'error': response.text
                    })
                    print(f"    âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                    
            except Exception as e:
                results.append({
                    'inference_time': time.time() - start_time,
                    'text': '',
                    'success': False,
                    'error': str(e)
                })
                print(f"    âŒ å¼‚å¸¸: {e}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in results if r['success']]
        if successful_results:
            inference_times = [r['inference_time'] for r in successful_results]
            return {
                'total_tests': num_tests,
                'successful_tests': len(successful_results),
                'first_inference_time': results[0]['inference_time'] if results[0]['success'] else None,
                'avg_inference_time': sum(inference_times) / len(inference_times),
                'min_inference_time': min(inference_times),
                'max_inference_time': max(inference_times),
                'all_results': results
            }
        else:
            return {
                'total_tests': num_tests,
                'successful_tests': 0,
                'error': 'All tests failed',
                'all_results': results
            }
    
    def stop_server(self) -> Dict:
        """åœæ­¢æœåŠ¡å™¨å¹¶è·å–æ€§èƒ½æ•°æ®"""
        print(f"\nğŸ›‘ åœæ­¢ {self.server_name} æœåŠ¡å™¨...")
        
        # åœæ­¢æ€§èƒ½ç›‘æ§
        perf_data = self.monitor.stop_monitoring()
        
        # ç»ˆæ­¢æœåŠ¡å™¨è¿›ç¨‹
        if self.process:
            self.process.terminate()
            try:
                self.process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.process.kill()
                self.process.wait()
        
        return perf_data

class PerformanceComparison:
    def __init__(self, audio_file: str):
        self.audio_file = audio_file
        self.results = {}
        
        # é…ç½®æµ‹è¯•æœåŠ¡å™¨
        self.servers = {
            'ONNX': ServerTester(
                '/Users/yangdongju/Desktop/code_project/pc/Ohoo/python-service/server.py',
                'ONNX (ä¼˜åŒ–æ¨¡å‹)'
            ),
             'PyTorch': ServerTester(
                '/Users/yangdongju/Desktop/code_project/pc/Ohoo/python-service/server_torch.py',
                'PyTorch (.ptæ¨¡å‹)'
            )
        }
    
    def run_comparison(self, num_inference_tests: int = 5):
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
        print("=" * 60)
        print("ğŸ SenseVoice æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¼€å§‹")
        print("=" * 60)
        
        for server_type, tester in self.servers.items():
            print(f"\n{'='*20} {server_type} æµ‹è¯• {'='*20}")
            
            try:
                # 1. æµ‹è¯•å¯åŠ¨æ€§èƒ½
                startup_result = tester.start_server()
                if not startup_result:
                    print(f"âŒ {server_type} å¯åŠ¨å¤±è´¥ï¼Œè·³è¿‡æµ‹è¯•")
                    continue
                
                # 2. æµ‹è¯•æ¨ç†æ€§èƒ½
                inference_result = tester.test_inference(self.audio_file, num_inference_tests)
                
                # 3. è·å–æ€§èƒ½ç›‘æ§æ•°æ®
                perf_data = tester.stop_server()
                
                # ä¿å­˜ç»“æœ
                self.results[server_type] = {
                    'startup': startup_result,
                    'inference': inference_result,
                    'performance': perf_data
                }
                
                print(f"âœ… {server_type} æµ‹è¯•å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {server_type} æµ‹è¯•å¤±è´¥: {e}")
                tester.stop_server()
                continue
            
            # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•ä¸‹ä¸€ä¸ª
            time.sleep(3)
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("="*60)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        
        for server_type, data in self.results.items():
            startup = data.get('startup', {})
            inference = data.get('inference', {})
            performance = data.get('performance', {})
            
            comparison_data.append({
                'æ¨¡å‹ç±»å‹': server_type,
                'å¯åŠ¨æ—¶é—´(s)': f"{startup.get('startup_time', 0):.2f}",
                'æ¨¡å‹åŠ è½½æ—¶é—´(s)': f"{startup.get('model_load_time', 0):.2f}",
                'é¦–æ¬¡æ¨ç†æ—¶é—´(s)': f"{inference.get('first_inference_time', 0):.2f}",
                'å¹³å‡æ¨ç†æ—¶é—´(s)': f"{inference.get('avg_inference_time', 0):.2f}",
                'æœ€å¿«æ¨ç†æ—¶é—´(s)': f"{inference.get('min_inference_time', 0):.2f}",
                'å¹³å‡å†…å­˜ä½¿ç”¨(MB)': f"{performance.get('avg_memory', 0):.0f}",
                'å³°å€¼å†…å­˜ä½¿ç”¨(MB)': f"{performance.get('max_memory', 0):.0f}",
                'å¹³å‡CPUä½¿ç”¨(%)': f"{performance.get('avg_cpu', 0):.1f}%"
            })
        
        # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
        df = pd.DataFrame(comparison_data)
        print("\nğŸ“‹ è¯¦ç»†å¯¹æ¯”æ•°æ®:")
        print(df.to_string(index=False))
        
        # è®¡ç®—æ€§èƒ½æå‡
        if 'PyTorch' in self.results and 'ONNX' in self.results:
            pytorch_data = self.results['PyTorch']
            onnx_data = self.results['ONNX']
            
            print("\nğŸš€ ONNX vs PyTorch æ€§èƒ½æå‡:")
            
            # å¯åŠ¨æ—¶é—´æå‡
            pytorch_startup = pytorch_data['startup'].get('startup_time', 0)
            onnx_startup = onnx_data['startup'].get('startup_time', 0)
            if pytorch_startup > 0:
                startup_improvement = ((pytorch_startup - onnx_startup) / pytorch_startup) * 100
                print(f"  â€¢ å¯åŠ¨é€Ÿåº¦æå‡: {startup_improvement:.1f}%")
            
            # æ¨ç†æ—¶é—´æå‡
            pytorch_inference = pytorch_data['inference'].get('avg_inference_time', 0)
            onnx_inference = onnx_data['inference'].get('avg_inference_time', 0)
            if pytorch_inference > 0:
                inference_improvement = ((pytorch_inference - onnx_inference) / pytorch_inference) * 100
                print(f"  â€¢ æ¨ç†é€Ÿåº¦æå‡: {inference_improvement:.1f}%")
            
            # å†…å­˜ä½¿ç”¨å¯¹æ¯”
            pytorch_memory = pytorch_data['performance'].get('avg_memory', 0)
            onnx_memory = onnx_data['performance'].get('avg_memory', 0)
            if pytorch_memory > 0:
                memory_reduction = ((pytorch_memory - onnx_memory) / pytorch_memory) * 100
                print(f"  â€¢ å†…å­˜ä½¿ç”¨å‡å°‘: {memory_reduction:.1f}%")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        results_file = f"performance_test_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def create_visualizations(self):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
        if len(self.results) < 2:
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('SenseVoice æ€§èƒ½å¯¹æ¯”', fontsize=16)
        
        # 1. å¯åŠ¨æ—¶é—´å¯¹æ¯”
        startup_times = []
        labels = []
        for server_type, data in self.results.items():
            startup_time = data['startup'].get('startup_time', 0)
            startup_times.append(startup_time)
            labels.append(server_type)
        
        axes[0, 0].bar(labels, startup_times, color=['#ff7f0e', '#2ca02c'])
        axes[0, 0].set_title('å¯åŠ¨æ—¶é—´å¯¹æ¯”')
        axes[0, 0].set_ylabel('æ—¶é—´ (ç§’)')
        for i, v in enumerate(startup_times):
            axes[0, 0].text(i, v + 0.1, f'{v:.2f}s', ha='center')
        
        # 2. æ¨ç†æ—¶é—´å¯¹æ¯”
        inference_times = []
        for server_type, data in self.results.items():
            inference_time = data['inference'].get('avg_inference_time', 0)
            inference_times.append(inference_time)
        
        axes[0, 1].bar(labels, inference_times, color=['#ff7f0e', '#2ca02c'])
        axes[0, 1].set_title('å¹³å‡æ¨ç†æ—¶é—´å¯¹æ¯”')
        axes[0, 1].set_ylabel('æ—¶é—´ (ç§’)')
        for i, v in enumerate(inference_times):
            axes[0, 1].text(i, v + 0.01, f'{v:.3f}s', ha='center')
        
        # 3. å†…å­˜ä½¿ç”¨å¯¹æ¯”
        memory_usage = []
        for server_type, data in self.results.items():
            memory = data['performance'].get('avg_memory', 0)
            memory_usage.append(memory)
        
        axes[1, 0].bar(labels, memory_usage, color=['#ff7f0e', '#2ca02c'])
        axes[1, 0].set_title('å¹³å‡å†…å­˜ä½¿ç”¨å¯¹æ¯”')
        axes[1, 0].set_ylabel('å†…å­˜ (MB)')
        for i, v in enumerate(memory_usage):
            axes[1, 0].text(i, v + 10, f'{v:.0f}MB', ha='center')
        
        # 4. æ€§èƒ½ç›‘æ§æ—¶é—´åºåˆ—ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
        if 'ONNX' in self.results and self.results['ONNX']['performance']['timestamps']:
            onnx_perf = self.results['ONNX']['performance']
            axes[1, 1].plot(onnx_perf['timestamps'], onnx_perf['memory_usage'], 
                           label='ONNXå†…å­˜', color='#2ca02c')
            
            if 'PyTorch' in self.results and self.results['PyTorch']['performance']['timestamps']:
                pytorch_perf = self.results['PyTorch']['performance']
                axes[1, 1].plot(pytorch_perf['timestamps'], pytorch_perf['memory_usage'], 
                               label='PyTorchå†…å­˜', color='#ff7f0e')
            
            axes[1, 1].set_title('è¿è¡Œæ—¶å†…å­˜ä½¿ç”¨')
            axes[1, 1].set_xlabel('æ—¶é—´ (ç§’)')
            axes[1, 1].set_ylabel('å†…å­˜ (MB)')
            axes[1, 1].legend()
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        chart_file = f"performance_comparison_{timestamp}.png"
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_file}")
        
        # æ˜¾ç¤ºå›¾è¡¨
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    audio_file = "/Users/yangdongju/Downloads/å½•éŸ³è®°å½•_20250928_0006.wav"
    
    if not Path(audio_file).exists():
        print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        return
    
    print("ğŸ¯ SenseVoice æ€§èƒ½å¯¹æ¯”æµ‹è¯•å·¥å…·")
    print(f"ğŸµ æµ‹è¯•éŸ³é¢‘: {audio_file}")
    
    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    comparison = PerformanceComparison(audio_file)
    
    # è¿è¡Œæµ‹è¯•
    comparison.run_comparison(num_inference_tests=5)
    
    # ç”ŸæˆæŠ¥å‘Š
    comparison.generate_report()
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    try:
        comparison.create_visualizations()
    except ImportError:
        print("âš ï¸ matplotlib æœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

if __name__ == "__main__":
    main()